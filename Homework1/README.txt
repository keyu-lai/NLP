
Homework1 
Name: Keyu Lai
Uni: kl2844

----------------------------------------------------------------------------------------------------

PART A

1)
UNIGRAM natural -13.766408817
BIGRAM natural that -4.05889368905
TRIGRAM natural that he -1.58496250072

2)
output/A2.uni.txt: 1052.4865859
output/A2.bi.txt: 53.8984761198
output/A2.tri.txt: 5.7106793082

3)
output/A3.txt: 12.5516094886

4)
Yes. Perplexity is the the weighted average branching factor of a language model. Since linear interpolation averagely combines the best model(trigram) with two relatively bad models(unigram and bigram), we will get a model with a perplexity a little larger than that of the best.

5)
output/Sample2_scored.txt: 11.1670289158
output/Sample2_scored.txt: 1627571078.54

----------------------------------------------------------------------------------------------------

PART B

2)
TRIGRAM CONJ ADV ADP -2.9755173148
TRIGRAM DET NOUN NUM -8.9700526163
TRIGRAM NOUN PRT PRON -11.0854724592

4)
* * 0.0
Night NOUN -13.8819025994
Place VERB -15.4538814891
prime ADJ -10.6948327183
STOP STOP 0.0
_RARE_ VERB -3.17732085089

5)
Percent correct tags: 93.3249946254

6)
Percent correct tags: 87.9985146677

----------------------------------------------------------------------------------------------------

Part A time: 14.69 sec
Part B time: 124.95 sec


